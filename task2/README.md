##task2:

#####基本文本处理技能
***
1.1 分词的概念（分词的正向最大、逆向最大、双向最大匹配法）

* 中文分词(Chinese Word Segmentation) 指的是将一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。
* 现有的分词方法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。
* 基于字符串匹配的分词方法又称机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。
*  按照扫描方向的不同，字符串匹配分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，可以分为单纯分词方法和分词与词性标注相结合的一体化方法。常用的字符串匹配方法有如下几种：

&ensp;&ensp;&ensp;&ensp;（1）正向最大匹配法（从左到右的方向）；

&ensp;&ensp;&ensp;&ensp;（2）逆向最大匹配法（从右到左的方向）；

&ensp;&ensp;&ensp;&ensp;（3）最小切分（每一句中切出的词数最小）；

&ensp;&ensp;&ensp;&ensp;（4）双向最大匹配（进行从左到右、从右到左两次扫描）

&ensp;&ensp;&ensp;&ensp;这类算法的优点是速度快，时间复杂度可以保持在O（n）,实现简单，效果尚可；但对歧义和未登录词处理效果不佳。

* 基于理解的分词方法<br>基于理解的分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。

* 基于统计的分词方法是在给定大量已经分词的文本的前提下，利用统计机
器学习模型学习词语切分的规律（称为训练），从而实现对未知文本的切分。例如最大概率分词方法和最大熵分词方法等。随着大规模语料库的建立，统计机器学习方法的研究和发展，基于统计的中文分词方法渐渐成为了主流方法；<br>
主要的统计模型有：N元文法模型（N-gram），隐马尔可夫模型（Hidden Markov Model ，HMM），最大熵模型（ME），条件随机场模型（Conditional Random Fields，CRF）等sp;在实际的应用中，基于统计的分词系统都需要使用分词词典来进行字符串匹配分词，同时使用统计方法识别一些新词，即将字符串频率统计和字符串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，又利用了无词典分词结合上下文识别生词、自动消除歧义的优点。
######中文分词工具:
* 1.jieba分词是国内使用人数最多的中文分词工具<br>
jieba分词支持三种模式<br>
&ensp;&ensp;&ensp;&ensp;- 精确模式，试图将句子最精确地切开，适合文本分析；<br>
&ensp;&ensp;&ensp;&ensp;- 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；<br>
&ensp;&ensp;&ensp;&ensp;- 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。<br>
1.2 词、字符频率统计；（可以使用Python中的collections.Counter模块，也可以自己寻找其他好用的库）：详见words.py
#####概念
***
2.1 语言模型中unigram、bigram、trigram的概念；

* n-gram模型也称为n-1阶马尔科夫模型，它有一个有限历史假设：当前词的出现概率仅仅与前面n-1个词相关.
*  当n取1、2、3时，n-gram模型分别称为unigram、bigram和trigram语言模型。n-gram模型的参数就是条件概率
* 假设词表的大小为100,000，那么n-gram模型的参数数量为100,000^n
n越大，模型越准确，也越复杂，需要的计算量越大。最常用的是bigram，其次是unigram和trigram，n取≥4的情况较少

2.2 unigram、bigram频率统计；（可以使用Python中的collections.Counter模块，也可以自己寻找其他好用的库:详见laguage_model.py
#####文本矩阵化：要求采用词袋模型且是词级别的矩阵化
***
步骤有：
3.1 分词（可采用结巴分词来进行分词操作，其他库也可以）；
3.2 去停用词；构造词表。
3.3 每篇文档的向量化<br>

* 无论文本式中文还是英文，我们首先要把它转化为计算机认识的形式。转化为计算机认识的形式的过程叫文本向量化。

* 向量化的粒度我们可以分为几种形式：

&ensp;&ensp;&ensp;&ensp;1)以字或单词为单位，中文就是单个字，英文可以是一个单词。
以词为单位，就需要加入一个分词的过程。分词算法本身是一个NLP中重要的基础课题，本文不详细讲解。<br>
&ensp;&ensp;&ensp;&ensp;2)以句子为单位，提炼出把一句话的高层语义，简而言之就是寻找主题模型。当然如果我们已经拿到了一句话的所有词的向量，也可以简单的通过取平均活着其他方式来代表这个句子。
下面我们主要介绍以词为单位的文本向量化方法，词集模型、词代模型、n-gram、TF-IDF、word2vec。和以句子为单位的主题模型，LSA、NMF、pLSA、LDA等。

* 词集模型和词代模型都是将所有文本中单词形成一个字典vocab，然后根据字典来统计单词出现频数。不同的是：

* 词集模型是单个文本中单词出现在字典中，就将其置为1，而不管出现多少次。
词代模型是单个文本中单词出现在字典中，就将其向量值加1，出现多少次就加多少次。
词集模型和词代模型都是基于词之间保持独立性，没有关联为前提。这使得其统计方便，但同时也丢失了文本间词之间关系的信息。
* 词袋模型假设我们不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。

* 与词袋模型非常类似的一个模型是词集模型(Set of Words,简称SoW)，和词袋模型唯一的不同是它仅仅考虑词是否在文本中出现，而不考虑词频。也就是一个词在文本在文本中出现1次和多次特征处理是一样的。在大多数时候，我们使用词袋模型。

* 词袋模型的三部曲：分词（tokenizing），统计修订词特征值（counting）与标准化（normalizing）。

